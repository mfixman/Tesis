% !TEX root = tesis.tex

\chapter{Comparison with Other Inference Methods}
\label{sec:comparison}

In this chapter, we compare the results of the Bayesian method presented in \cref{sec:inference_methodology} with common machine learning methods using several feature extraction methods in the graph used for this study.
Like in the previous chapter, the idea is to know whether each user $v$ belongs to the lower socioeconomic strata $H_1$ or the higher $H_2$ by knowing the distribution of calls and text messages to his contacts.

This classifier creates a proper baseline for other comparisons. These will be used in the dataset $G = \left< V, E \right>$ presented the bank data contained in $B^{\train}$, and will be used to provide either information about the users.
Unlike the method presented in the previous chapter, we can optionally forego the necessity for having at least one neighbour with known socioeconomic index for each user.
For this reason, we experiment with these methods with two possible inputs, both previously defined in \cref{sec:data_partitioning}: one that uses the entire set of nodes, called the \emph{Outer Graph} or $\Omega$, and another one that uses only the nodes which have at least one neighbour with known socioeconomic index, called the \emph{Inner Graph} or $\Upsilon$.
Additionally, as explained in the referred section, the data is conveniently partitioned into training and testing sets to have a similar amount of data than in a real case.

The tables in \cref{sec:comparison_results} compare the results of these methods with the \emph{Bayesian Algorithm} described in \cref{sec:inference_methodology} along several metrics.

\section{Random Selection}
\label{subsec:random_selection}

One of the simplest methods for solving many classification problems is using random selection. This classifier simply chooses randomly to which strata each user belongs.

\begin{equation}
\label{eq:random}
\begin{aligned}
	P \left( v \in H_1 \right) &= 0.5 \\
	P \left( v \in H_2 \right) &= 0.5
\end{aligned}
\end{equation}

This produces a good baseline for comparing other inference methods.

\section{Majority Voting}
\label{subsec:majority_voting}

The method of \emph{Majority Voting} is a basic but powerful way of inferring which category a user belongs to.
It simply chooses the category of each user $v \in V$ as the most common category within its contacts.

In case of a tie (which happens often when using the \emph{Outer Graph}, as many users don't have any neighbour with a known category), the category is chosen randomly.
This approach can be formalized as in \cref{eq:majority_voting}, where $\contacts^{\low}_{v}$ and $\contacts^{\high}_{v}$ have the same definitions as in \ref{sec:inference_methodology}: the amount of contacts user $v$ has of either low or high socioeconomic index.

\begin{equation}
\label{eq:majority_voting}
	P \left( v \in H_1 \right) =
\begin{cases}
	0   & \text{if} \ \contacts^{\low}_{v} < \contacts^{\high}_{v} \\
	0.5 & \text{if} \ \contacts^{\low}_{v} = \contacts^{\high}_{v} \\
	1   & \text{if} \ \contacts^{\low}_{v} > \contacts^{\high}_{v}
\end{cases}
\end{equation}

\section{Methods Based in Machine Learning}
\label{subsec:methods_ml}

The following methods use commonly used supervised Machine Learning algorithms, described in \cref{subsec:supervised_machine_learning}, used with many similar \emph{Feature Extraction} methods on $G$.

The initial feature extraction method, referred to as \emph{User Data}, is described in \cref{subsec:user_data} and marked as $\ego_0$, consists of accumulating the total information about the links neighboring some user $v \in V$. Later, as shown in \cref{subsec:higherorderuserdata}, it's possible to accumulate links from more levels of the \emph{Ego Network} to create feature sets that be used to build a better prediction of the data.

An extra method for feature extraction is presented in \cref{subsec:categoricaluserdata}, which in addition to doing the extraction of other methods it accumulates separately edges that go to \emph{High Income} and \emph{Low Income} users.

The features of each method are merged with the ones from the immediate predecessor methods, as shown in the graph in \cref{fig:mlrelationships}, while the amount of features in each level is described in \cref{tab:datasettable}.

\begin{figure}
\centering
\resizebox{!}{.3\textheight}{%
	\framebox{%
		\input{figures/mlrelationships.tikz}
	}
}
\caption{Relationships between the \emph{Feature Extraction} methods of \cref{subsec:methods_ml}. \textcolor{Blue}{blue} edges represent an increase in \emph{Ego Network} size, a process which is describe in \cref{subsec:higherorderuserdata}, while \textcolor{ForestGreen}{green} edges represent adding label information, which is described in \cref{subsec:categoricaluserdata}}
\label{fig:mlrelationships}
\end{figure}

\begin{table}
\centering
\begin{tabular}{>{\bfseries}l r}
\toprule
Level & Features \\
\midrule
$\ego_0$ & \num{8}  \\
$\ego_1$ & \num{16} \\
$\ego_2$ & \num{24} \\
$\cat_0$ & \num{24} \\
$\cat_1$ & \num{48} \\
$\cat_2$ & \num{72} \\
\bottomrule
\end{tabular}
\caption{Number of features used for testing each Machine Learning model on each level}
\label{tab:datasettable}
\end{table}

\subsection{User Data --- Level $\ego_0$}
\label{subsec:user_data}

The features on the graph $G = \left< V, E \right>$ can be accumulated for all users in a manner similar to the one in \cref{subsec:feature_accumulation}, presented again in \cref{eq:bayesian_calls_accum_copy}.

\begin{align}
\label{eq:bayesian_calls_accum_copy}
& \calls^{\low}_v = \sum_{\substack{e \in E \\ e_d = v \\ e_o \in H_1}}{e_c} + \sum_{\substack{e \in E \\ e_o = v \\ e_d \in H_1}}{e_c}
& \calls^{\high}_v = \sum_{\substack{e \in E \\ e_d = v \\ e_o \in H_2}}{e_c} + \sum_{\substack{e \in E \\ e_o = v \\ e_d \in H_2}}{e_c} \\
& \etime^{\low}_v = \sum_{\substack{e \in E \\ e_d = v \\ e_o \in H_1}}{e_t} + \sum_{\substack{e \in E \\ e_o = v \\ e_d \in H_1}}{e_t}
& \etime^{\high}_v = \sum_{\substack{e \in E \\ e_d = v \\ e_o \in H_2}}{e_t} + \sum_{\substack{e \in E \\ e_o = v \\ e_d \in H_2}}{e_t} \\
& \sms^{\low}_v = \sum_{\substack{e \in E \\ e_d = v \\ e_o \in H_1}}{e_s} + \sum_{\substack{e \in E \\ e_o = v \\ e_d \in H_1}}{e_s}
& \sms^{\high}_v = \sum_{\substack{e \in E \\ e_d = v \\ e_o \in H_2}}{e_s} + \sum_{\substack{e \in E \\ e_o = v \\ e_d \in H_2}}{e_s}
\end{align}
\begin{equation}
\begin{aligned}
	\contacts^{\low }_v &= &\left| \left\{ e \in E \mid e_o = v \land e_d \in H_1 \right\} \cup \left\{ e \in E \mid e_d = v \land e_o \in H_1 \right\} \right| \\
	\contacts^{\high }_v &= &\left| \left\{ e \in E \mid e_o = v \land e_d \in H_2 \right\} \cup \left\{ e \in E \mid e_d = v \land e_o \in H_2 \right\} \right|
\end{aligned}
\end{equation}

However, and unlike in the experiments in \ref{sec:inference_methodology}, this time we aren't constrained either by having only users which have contacts with other users with known \emph{Income Category}, nor with having to have only two features for each category which was necessary since the solution used the \emph{Beta Distribution}.

This way, it's possible to accumulate features for each user $v \in V$ in the manner shown by \cref{eq:user_data}.

\begin{equation}
\label{eq:user_data}
\begin{gathered}
\begin{aligned}
\incalls_v &= \sum_{\substack{e \in E \\ e_d = v}} \calls_e &
\outcalls_v &= \sum_{\substack{e \in E \\ e_o = v}} \calls_e \\
\intime_v &= \sum_{\substack{e \in E \\ e_d = v}} \etime_e &
\outtime_v &= \sum_{\substack{e \in E \\ e_o = v}} \etime_e \\
\insms_v &= \sum_{\substack{e \in E \\ e_d = v}} \sms_e &
\outsms_v &= \sum_{\substack{e \in E \\ e_o = v}} \sms_e \\
\end{aligned} \\
\begin{aligned}
\incontacts_v &= \left| \left\{ e \in E \mid e_d = v \right\} \right| \\
\outcontacts_v &= \left| \left\{ e \in E \mid e_o = v \right\} \right|
\end{aligned}
\end{gathered}
\end{equation}

These features will be referred as the \emph{User Data} of user $v$.
There features contain information about the \emph{Neighborhood} of $\upsilon$, also referred to as the \emph{Ego Network of Distance 1}. This definition is used later in this chapter, and the nodes belonging to the \emph{Neighborhood} of $\upsilon$ can be formally defined as in \cref{eq:neighbourhood}.

\begin{equation}
\label{eq:neighbourhood}
\neigh \left( \upsilon \right) = \left\{ e_o \mid e \in E \land e_d = \upsilon \right\} \cup \left\{ e_d \mid e \in E \land e_o = \upsilon \right\}
\end{equation}

\subsection{Categorical User Data --- Level $\cat_0$}
\label{subsec:categoricaluserdata}

A possible feature set consists of separating the data of the neighborhood of each user $\upsilon \in \Upsilon$ into two disjoint groups, $L_{\upsilon}$ and $K_{\upsilon}$, which contain the neighbors of $\upsilon$ in the \emph{Low Income} and \emph{High Income} categories of income respectively\footnotemark{}.

\footnotetext{Note that, since not all users have banking information, there may be nodes in the neighborhood of $\upsilon$ which don't belong to either $L_{\upsilon}$ or $K_{\upsilon}$.}

\begin{equation}
\label{eq:cud_categories}
\begin{aligned}
	L_{\upsilon} &= H_1 \cap \neigh \left( \upsilon \right) \\
	K_{\upsilon} &= H_2 \cap \neigh \left( \upsilon \right)
\end{aligned}
\end{equation}

Having defined these groups it's possible to define a set of features similar to the one in \cref{subsec:user_data}, where each feature is separated by the category of the neighbor. \Cref{eq:matcatuserdata} represents an intuitive way to define the names of the new features.

\begin{equation}
\label{eq:matcatuserdata}
	\begin{Bmatrix} \text{in} \\ \text{out} \end{Bmatrix}
	\times
	\begin{Bmatrix} \text{calls} \\ \text{time} \\ \text{sms} \\ \text{contacts} \end{Bmatrix}
	\times
	\begin{Bmatrix} \text{low} \\ \text{high} \end{Bmatrix}
\end{equation}

\Cref{eq:categoricaluserdata,eq:categoricaluserdata2} contain the way to calculate those features. Since the number of individual features is high and the formulas are similar and repetitive, some generalizations were added for the simplest ones.

\begin{equation}
\label{eq:categoricaluserdata}
\begin{gathered}
	\text{For $\varpi$ being one of $\left\{ \calls, \etime, \sms \right\}$,} \\
	% \left( \forall \varpi \in \left\{ \calls, \etime, \sms \right\} \right) \\
\begin{aligned}
	\underline{\text{in}{\varpi}\text{low}}_v = \sum_{\substack{e \in E \\ e_d \in L_v \\ e_o = v}} &\varpi_e &
	\underline{\text{in}{\varpi}\text{high}}_v = \sum_{\substack{e \in E \\ e_d \in K_v \\ e_o = v}} &\varpi_e \\
	\underline{\text{out}{\varpi}\text{low}}_v = \sum_{\substack{e \in E \\ e_o \in L_v \\ e_d = v}} &\varpi_e &
	\underline{\text{out}{\varpi}\text{high}}_v = \sum_{\substack{e \in E \\ e_o \in K_v \\ e_d = v}} &\varpi_e \\
\end{aligned}
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:categoricaluserdata2}
\begin{aligned}
	\incontactslow_v   &= \left| \left\{ e \in E \mid e_d = v \land e_o \in L_v \right\} \right| \\
	\incontactshigh_v  &= \left| \left\{ e \in E \mid e_d = v \land e_o \in K_v \right\} \right| \\
	\outcontactslow_v  &= \left| \left\{ e \in E \mid e_o = v \land e_d \in L_v \right\} \right| \\
	\outcontactshigh_v &= \left| \left\{ e \in E \mid e_o = v \land e_d \in K_v \right\} \right|
\end{aligned}
\end{equation}

Unlike the features in \cref{subsec:user_data}, and like the method presented in \cref{sec:inference_methodology}, the features in this section will be be different when testing between the nodes of $\Omega$ (the \emph{Outer Graph}) and $\Upsilon$ (the \emph{Inner Graph}).

\subsection{Higher Order User Data --- Level $\ego_{n > 0}$}
\label{subsec:higherorderuserdata}

The features described in \cref{subsec:user_data} correspond to the information about calls and SMS from an user $\upsilon \in \Upsilon$ towards all of its neighbors, which is described as the \emph{Ego Network of Distance 1}. However, there's no reason why this information can't be extended to other nodes at a higher distance from $\upsilon$.

If the distance between two nodes is defined using the intuitive definition presented in \cref{eq:distance}, it's possible to define the \emph{User Data of Order $n$} for any number $n \in \mathbb{N}$ as the accumulation of calls and SMS where one endpoint is on the border of the \emph{Ego Network of Order $n$} and the other one isn't. The \emph{Ego Network of Order $n$} or a certain node $\upsilon$ is the sub-graph composed of the node $\upsilon$ and all the nodes which are at at most distance $n$ of $\upsilon$.

\begin{equation}
d \left( a, b \right) =
\begin{cases}
	0 & \text{if } a = b \\
	1 + \min_{v \in \ego \left(b \right)} d \left( a, v \right) & \text{otherwise}
\end{cases}
\label{eq:distance}
\end{equation}

This method creates a set of \emph{Higher Order Features} similar to the ones in \cref{eq:user_data}, with the exception that the values of the edges summed for each node are defined on the distance instead of on the definition of the $E$ itself.
Indeed, for any graph $G = \left< V, E \right>$ we can define features with formulas similar to the ones in \cref{eq:higher_order_user_data}.

\begin{equation}
	\begin{aligned}
		\incalls^n_v = \sum_{\subalign{e \in E \\ d \left( e_o, v \right) &= n \\ d \left( e_d, v \right) &= n + 1 }} \calls_e &
		\hfill &
		\outcalls^n_v = \sum_{\subalign{e \in E \\ d \left( e_d, v \right) &= n \\ d \left( e_o, v \right) &= n + 1 }} \calls_e \\
	\end{aligned}
\label{eq:higher_order_user_data}
\end{equation}

This definition can be seen more intuitively in the graph in \cref{fig:higherorderuserdata}.

\begin{figure}
\centering
\framebox{%
	\input{figures/ego.tikz}
}
\caption{Example of the edges present in the calculation of the \emph{Higher Order User Data} for a certain node $v$. \textcolor{red}{Red} edges represent edges whose features are accumulated in the \emph{User Data of Order 0}, \textcolor{blue}{blue} edges represent those of \emph{Order 1}, and \textcolor{ForestGreen}{green} those of \emph{Order 2}.}
\label{fig:higherorderuserdata}
\end{figure}

Every set $\ego_n$ contains features from that level and all the previous ones as presented formally in \cref{eq:formal_higher_order}. This implies that, if the prediction algorithms are smart enough, the result of using the features from $\ego_{n + 1}$ should be at least as good as using the ones from $\ego_n$.

\begin{equation}
	(\forall n \in \mathbb{N}) \ego_n \subset \ego_{n + 1}
\label{eq:formal_higher_order}
\end{equation}

In one of the initial experiments in this thesis we added more features related to the $\left\{ \text{in}, \text{out} \right\}$ group to indicate how many ``in'' or ``out'' edges must be traversed from $\upsilon$. However, this added an exponentially large new amount of features without much significant data, and thus was left out from the following experiments.

\subsection{Higher Order Categorical User Data --- Level $\cat_{n > 0}$}

It's possible to combine the ideas in \cref{subsec:categoricaluserdata,subsec:higherorderuserdata} to create a group of sets of features containing the data from the border of the \emph{Ego Network or Order $n$}, separated by two disjoint groups depending in the category of the node outside the ego network, as in \cref{eq:formal_higher_order_categorical}, with $E$ defined as in \cref{sec:dataset} as the set of edges where $e_o$ is the user that originated a call and $e_d$ the user in the destination.

\begin{equation}
	\begin{aligned}
		\incallslow^n_v = \sum_{\subalign{e \in &E \\ e_d \in &H_1 \\ d \left( e_o, v \right) &= n \\ d \left( e_d, v \right) &= n + 1}} \calls_e &\quad
		\incallshigh^n_v = \sum_{\subalign{e \in &E \\ e_d \in &H_2 \\ d \left( e_o, v \right) &= n \\ d \left( e_d, v \right) &= n + 1}} \calls_e \\
		\outcallslow^n_v = \sum_{\subalign{e \in &E \\ e_o \in &H_1 \\ d \left( e_d, v \right) &= n \\ d \left( e_o, v \right) &= n + 1}} \calls_e &\quad
		\outcallshigh^n_v = \sum_{\subalign{e \in &E \\ e_o \in &H_2 \\ d \left( e_d, v \right) &= n \\ d \left( e_o, v \right) &= n + 1}} \calls_e \\
	\end{aligned}
\label{eq:formal_higher_order_categorical}
\end{equation}

Using these features we can have a more complete understanding of the data surrounding each node $v$.
In \cref{sec:comparison_results}, where we present the final results, we prove that the best predictor between all the methods explored in this chapter is this same method defining when using a \emph{Random Forest} to predict the final labels, both when using the inner graph (and using only the ego network when $n = 1$) and when using the entire outer graph (and using the ego network with $n = 2$).

The amount of features grows exponentially in each step (see \cref{tab:datasettable}), and so does the time spent in each computation. Since the best results are found with metrics when looking at the \emph{Ego Network} of level 1 of the inner graph (that is, the information contained in the edges of the neighbours of the neighbours of each node), which can be explained by the fact that the noise of the information about users far away from the noise has more impact than the useful information, no tests were attempted in cases where $n \geq 3$.

\section{Machine Learning Methods}

All the feature sets described in the previous sections are individually going to be used as the input objects of the \emph{Training Data} and either the \emph{Outer Graph} $\Omega$ or the \emph{Inner Graph} $\Upsilon$ will be used as the output with several \emph{Supervised Machine Learning} methods. The result of these methods will be measured using many metrics of the results, previously described in \cref{subsec:mlmetrics}, and compared against other methods (including the Bayesian Algorithm of \cref{sec:inference_methodology}) in \cref{sec:comparison_results}.

To prevent the problem of \emph{Overfitting} the results are generated by using \emph{Cross-Validated} estimates of the data using \emph{K-Folds} with $K = 5$. This way, each quintile of the data is predicted using only data from the other four.

The two methods used in this thesis, \emph{Logistic Regression} and \emph{Random Forest} tend to have different variance in the results~\cite{ting2016}, therefore different sources of errors may end up decreasing the models accuracy differently. This is a good for our model, since it means that a single source of errors has less probability of affecting either predictor.

These \emph{Machine Learning} methods used in this section contain many hyperparameters which may affect the result, and calculating the optimal value for them isn't trivial. For this reason this experiment includes a \emph{Grid Search} of the data against all possible hyperparameters. The resulting hyperparameters of the \emph{Grid Search} are presented in \cref{tab:gridsearch}.

\begin{table}
\centering
\begin{tabular}{>{\bfseries}c >{\bfseries}l >{\hspace{3em}}r r r >{\hspace{1em}}r r r}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Level} & \multicolumn{3}{>{\hspace{1em}}c}{\textbf{Logistic Regression}} & \multicolumn{3}{>{\hspace{2em}}c}{\textbf{Random Forest}} \\
&& \phantom & $\log_{10}{\left(C\right)}$ && Criterion & Max Features & Replacement \\
\midrule
\multirow{5}{*}{$\Upsilon$}
& $\ego_0$ & & $-2$ & & Entropy &  $\sqrt{f}$ & True \\
& $\ego_1$ & & $-2$ & & Entropy &         $f$ & True \\
& $\ego_2$ & & $-2$ & & Entropy &         $f$ & True \\
& $\cat_0$ & & $-3$ & & Entropy & $\log_2{f}$ & True \\
& $\cat_1$ & &  $0$ & & Entropy &         $f$ & True \\
& $\cat_2$ & & $-1$ & & Entropy &         $f$ & True \\
[2ex]
\multirow{5}{*}{$\Omega$}
& $\ego_0$ & &  $0$ & & Gini    & $\log_2{f}$ & True \\
& $\ego_1$ & &  $1$ & & Entropy & $\log_2{f}$ & True \\
& $\ego_2$ & &  $0$ & & Entropy &  $\sqrt{f}$ & True \\
& $\cat_0$ & & $-2$ & & Entropy & $\log_2{f}$ & True \\
& $\cat_1$ & &  $2$ & & Gini    &  $\sqrt{f}$ & True \\
& $\cat_2$ & &  $2$ & & Entropy & $\log_2{f}$ & True \\
\bottomrule
\end{tabular}
\caption{Best hyperparameters for each group of features in each model used for predicting the result.}
\label{tab:gridsearch}
\end{table}

\subsection{Logistic Regression}

The method of \emph{Logistic Regression}, which is described with more detail in \cref{subsec:logisticregression}, consists of doing some regression analysis with the data after applying some \emph{Logistic Function} to normalize the data.

The most important hyperparameter of this data is the \emph{Regularization Factor} $C$, which specifies the regularization of the input. As shown in \cref{eq:gridsearchlogreg}, this value is searched in exponential increments.

\begin{equation}
\label{eq:gridsearchlogreg}
C \in \left\{ 10^{-3}, 10^{-2}, 10^{-1}, 10^0, 10^1, 10^2, 10^3 \right\}
\end{equation}

Since the different data aren't linearly distributed, the input is \emph{Regularized} before using this method so that, when running the model, each column has its mean in $\mu = 0$ and its variance $\sigma^2 = 1$. This makes the model more robust~\cite{mitchellml1997}.

\subsection{Random Forest}
\label{subsec:comparison_random_forest}

The method of \emph{Random Forest}, which is described with more detain in \cref{subsec:randomforest}, consists of constructing a multitude of \emph{Decision Trees} and outputting the class that is the \emph{mode} of the classes.

There are several hyperparameters used in this method, namely the \texttt{Criterion} to measure the quality of a split (\texttt{gini} uses Gini impurity, while \texttt{entropy} uses information gain), the function used to calculate the \texttt{Sample Size} given the amount of features $f$, and whether the samples are taken with or without \texttt{Replacement}. This is formalized in \cref{eq:gridsearchrandomforest}.

Another possible hyperparameter would be the number of trees. However, as it's been shown in~\cite{breiman2001random}, \emph{Random Forests} converge quickly for a high amount of trees. Since the objective of this section isn't to optimize by time, the value will be set to the sufficiently high $\text{\texttt{n\_estimators}} = 50$.

\begin{equation}
\label{eq:gridsearchrandomforest}
\begin{aligned}
	\operatorname{Criterion} &\in \left\{ \operatorname{Gini}, \operatorname{Entropy} \right\} \\
	\operatorname{Features} &\in \left\{ f, \sqrt{f}, \log_2{f} \right\} \\
	\operatorname{Replacement} &\in \left\{ \operatorname{True}, \operatorname{False} \right\}
\end{aligned}
\end{equation}

\section{Validation Metrics}
\label{subsec:validationmetrics}
There are several validation metrics used for each method.

\begin{description}
	\item[Accuracy] as described in \cref{subsec:accuracy}, which measures the general performance of this method.
	\item[Precision] as described in \cref{subsec:precisionrecall}, which measures the performance regarding the positive instances found by this method.
	\item[Recall] as described in \cref{subsec:precisionrecall}, which measures the performance regarding the positive instances in the dataset.
	\item[Area Under the Curve] as described in \cref{subsec:auc}, which measures the general performance disregarding which threshold is used.
	\item[$\mathbf{F_1}$ Score] as described in \cref{subsec:fmeasure} which is generalized score balancing Precision and Recall.
	\item[$\mathbf{F_4}$ Score] as described in \cref{subsec:fmeasure}, which gives more weight to the Recall. This is usually wanted since the ultimate practical objective of this study is to find wealthier people, even if the result has low Precision.
	\item[Fit Time] is the time it takes to fit a model. It's particularly high in ensemble methods such as \emph{Random Forest}.
	\item[Predict Time] can be used to break ties between similar models.
\end{description}

\section{Results}
\label{sec:comparison_results}

\Cref{tab:innercomparison,tab:outercomparison} show various metrics which result from applying the methods described in this Section to the datasets presented in this thesis, after using the best hyperparameters shown in \cref{tab:gridsearch} and applying 5-fold \emph{Cross Validation} to prevent \emph{Overfitting}. This result is compared to the \emph{Bayesian Algorithm} presented in \cref{sec:inference_methodology}.

By comparing the methods based in \emph{Machine Learning} we can reach a conclusion consistent with~\cite{muchlinski2016}, where the methods based in \emph{Random Forest} tend to perform better in real-world scenarios than the ones based in \emph{Logistic Regression}.

Increasing the breadth of the \emph{Ego Network} by one level slightly improves the metrics on the \emph{Machine Learning} methods\footnotemark[1]{} when using with \emph{Random Forest} learning, which is more versatile to odd cases and non-linear data~\cite{logisticvsdecision} However, the model isn't improved further by going further in the \emph{Ego Network} since it would add noise to the features, making the prediction less accurate\footnotemark[2]{}. Indeed, it's possible to see the fallibility of common \emph{Random Forest} fitting methods, since the fact that $\ego_2 \supset \ego_1$ should be enough to create a model that at least as good as the previous one.

% \footnotetext[1]{Compare the rows of $\ego_0$ to the ones with $\ego_1$}
% \footnotetext[2]{Compare the rows of $\ego_1$ to the ones with $\ego_2$}

Additionally, adding categorical information to the feature extraction as in \cref{subsec:categoricaluserdata} to the \emph{Ego Network} of distance 1 and distance 2\footnotemark[3]{} results in the greatest improvement in most metrics. These improvements don't hold up when going one level further in the \emph{Ego Network}, as can be seen that the rows referring to the accumulated categorical data on that level has lower metrics than the previous one\footnotemark[4]{}. We hypothesize with strong certainty that the metrics will keep decreasing when adding more levels to the \emph{Ego Network}.

% \footnotetext[3]{Compare the rows of $\cat_0$ to the ones with $\cat_1$}
% \footnotetext[4]{Compare the rows of $\cat_1$ to the ones with $\cat_2$}

The results in \cref{tab:innercomparison} show that, when using the data from the labelled users $\Upsilon$, the \emph{Bayesian Algorithm} presented in \cref{sec:inference_methodology} has a significantly higher \emph{Area Under the Curve} than both the naÃ¯ve algorithms and the ones based in \emph{Machine Learning}. This, in addition to the fact that making a prediction in the algorithm is faster than fitting the \emph{Random Forest} methods (which give the best metrics for the \emph{Machine Learning} ones), and that the amount of data needed for running the algorithm is much lower (2 columns vs.\ the amounts shown in \cref{tab:datasettable}) and much simpler to construct, makes it more suitable for production than any of the more standard algorithms, which are based in regular \emph{Machine Learning} methods.

\newpage

\begin{table}[t]
\centering
\begin{tabular}{>{\bfseries}l >{\bfseries}l >{\hspace{1ex}} r r r r r r r r}
\toprule
\ct{Model} & \ct{Level} & \ct{Acc.} & \ct{Prec.} & \ct{Rec.} & \ct{AUC} & \ct{F\textsubscript{1}} & \ct{F\textsubscript{4}} & \ct{t\textsubscript{fit}} & \ct{t\textsubscript{pred}} \\
\midrule

\multicolumn{2}{>{\bfseries}l}{Random}
& 0.499 & 0.499 & 0.500 & 0.499 & 0.500 & 0.500 & \ct{\NA} & \SI{0.005}{\second} \\

\multicolumn{2}{>{\bfseries}l}{Majority}
& 0.681 & 0.640 & 0.826 & 0.681 & 0.721 & 0.712 & \ct{\NA} & \SI{0.059}{\second} \\

\midrule

\multicolumn{2}{>{\bfseries}l}{Bayesian}
& 0.693 & 0.665 & 0.792 & \textbf{0.746} & \textbf{0.723} & \textbf{0.783} & \ct{\NA} & \SI{33.155}{\second} \\

\midrule

\multirow{6}{*}{LR}
& $\ego_0$ & 0.536 & 0.531 & 0.625 & 0.536 & 0.574 & 0.619 & \SI{0.145}{\second}   & \SI{0.002}{\second} \\
& $\ego_1$ & 0.535 & 0.525 & 0.730 & 0.535 & 0.611 & 0.714 & \SI{0.141}{\second}   & \SI{0.011}{\second} \\
& $\ego_2$ & 0.568 & 0.578 & 0.525 & 0.569 & 0.550 & 0.528 & \SI{0.119}{\second}   & \SI{0.003}{\second} \\
& $\cat_0$ & 0.686 & 0.655 & 0.785 & 0.686 & 0.714 & 0.776 & \SI{0.167}{\second}   & \SI{0.005}{\second} \\
& $\cat_1$ & 0.693 & 0.665 & 0.780 & 0.693 & 0.718 & 0.772 & \SI{1.588}{\second}   & \SI{0.011}{\second} \\
& $\cat_2$ & 0.693 & 0.670 & 0.764 & 0.692 & 0.714 & 0.758 & \SI{0.956}{\second}   & \SI{0.009}{\second} \\
\midrule

\multirow{6}{*}{RF}
& $\ego_0$ & 0.548 & 0.548 & 0.550 & 0.548 & 0.549 & 0.550 & \SI{5.986}{\second}   & \SI{0.588}{\second} \\
& $\ego_1$ & 0.582 & 0.583 & 0.577 & 0.582 & 0.580 & 0.577 & \SI{56.548}{\second}  & \SI{0.483}{\second} \\
& $\ego_2$ & 0.576 & 0.577 & 0.580 & 0.576 & 0.579 & 0.580 & \SI{50.197}{\second}  & \SI{0.253}{\second} \\
& $\cat_0$ & 0.671 & 0.665 & 0.690 & 0.671 & 0.677 & 0.688 & \SI{6.346}{\second}   & \SI{0.539}{\second} \\
& $\cat_1$ & \textbf{0.714} & 0.713 & 0.716 & 0.714 & 0.714 & 0.716 & \SI{96.005}{\second}  & \SI{0.460}{\second} \\
& $\cat_2$ & 0.709 & 0.710 & 0.711 & 0.709 & 0.711 & 0.711 & \SI{81.528}{\second}  & \SI{0.242}{\second} \\
\bottomrule
\end{tabular}
\caption{Resulting metrics of different methods used in \cref{sec:results,sec:comparison} tested on the \emph{Inner Graph} $\Upsilon$, which contains only nodes which have at least one neighbour with socioeconomic information. \textbf{Bolded} items represent the highest value for each metric.}
\label{tab:innercomparison}
\end{table}

\begin{table}[b]
\centering
\begin{tabular}{ >{\bfseries}l >{\bfseries}l >{\hspace{1ex}} r r r r r r r r }
\toprule
\ct{Model} & \ct{Level} & \ct{Acc.} & \ct{Prec.} & \ct{Rec.} & \ct{AUC} & \ct{F\textsubscript{1}} & \ct{F\textsubscript{4}} & \ct{t\textsubscript{fit}} & \ct{t\textsubscript{pred}} \\
\midrule

\multicolumn{2}{>{\bfseries}l}{Random}
& 0.499 & 0.499 & 0.500 & 0.499 & 0.500 & 0.500 & \ct{\NA} & \SI{0.005}{\second} \\

\multicolumn{2}{>{\bfseries}l}{Majority}
& 0.565 & 0.747 & 0.197 & 0.565 & 0.312 & 0.206 & \ct{\NA} & \SI{0.204}{\second} \\

\midrule

\multirow{6}{*}{LR} &
  $\ego_0$ & 0.534 & 0.586 & 0.234 & 0.534 & 0.335 & 0.243 & \SI{0.937}{\second}   & \SI{0.016}{\second} \\
& $\ego_1$ & 0.547 & 0.617 & 0.250 & 0.547 & 0.356 & 0.260 & \SI{1.347}{\second}   & \SI{0.035}{\second} \\
& $\ego_2$ & 0.563 & 0.586 & 0.430 & 0.563 & 0.496 & 0.437 & \SI{1.055}{\second}   & \SI{0.023}{\second} \\
& $\cat_0$ & 0.565 & 0.746 & 0.198 & 0.565 & 0.313 & 0.207 & \SI{1.871}{\second}   & \SI{0.041}{\second} \\
& $\cat_1$ & 0.577 & 0.727 & 0.247 & 0.577 & 0.368 & 0.257 & \SI{9.816}{\second}   & \SI{0.077}{\second} \\
& $\cat_2$ & 0.589 & 0.636 & 0.415 & 0.589 & 0.503 & 0.424 & \SI{9.456}{\second}   & \SI{0.065}{\second} \\
\midrule

\multirow{6}{*}{RF} &
  $\ego_0$ & 0.543 & 0.544 & 0.529 & 0.543 & 0.536 & 0.530 & \SI{25.789}{\second}  & \SI{4.878}{\second} \\
& $\ego_1$ & 0.578 & 0.585 & 0.537 & 0.578 & 0.560 & 0.540 & \SI{102.961}{\second} & \SI{5.608}{\second} \\
& $\ego_2$ & 0.583 & 0.590 & 0.541 & 0.583 & 0.564 & 0.543 & \SI{70.447}{\second}  & \SI{3.148}{\second} \\
& $\cat_0$ & 0.568 & 0.573 & 0.536 & 0.568 & 0.554 & 0.538 & \SI{32.981}{\second}  & \SI{5.371}{\second} \\
& $\cat_1$ & 0.613 & 0.634 & 0.533 & 0.613 & 0.579 & 0.538 & \SI{44.911}{\second}  & \SI{6.002}{\second} \\
& $\cat_2$ & \textbf{0.614} & 0.635 & 0.534 & \textbf{0.614} & \textbf{0.580} & \textbf{0.539} & \SI{50.589}{\second}  & \SI{3.484}{\second} \\
\bottomrule
\end{tabular}
\caption{Resulting metrics of different methods used in \cref{sec:comparison} tested in the \emph{Outer Graph} $\Omega$, which includes all nodes. \textbf{Bolded} items represent the highest value for each metric.}
\label{tab:outercomparison}
\end{table}
